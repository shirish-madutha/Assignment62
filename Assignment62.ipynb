{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b36eb-619d-4d7d-9d5e-2b0d785044c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q1. What is the KNN algorithm? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" KNN, or k-Nearest Neighbors, is a simple and widely used algorithm for classification and regression tasks in machine learning. It's a type of instance-based learning, where the algorithm makes predictions based on the majority class or average of the k-nearest data points in the feature space. The \"k\" in KNN represents the number of neighbors considered when making predictions. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581459a1-fd66-4876-a279-4174ca88bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q2. How do you choose the value of K in KNN? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" The choice of the value of K in KNN is crucial. A small K may lead to noise sensitivity, while a large K may make the algorithm less sensitive to local patterns. The optimal K depends on the dataset and the specific problem. Cross-validation and grid search are common techniques to find the best K by evaluating the model's performance on different values of K. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedcac67-c3ef-40d1-b79b-67ff2bea3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q3. What is the difference between KNN classifier and KNN regressor? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" KNN Classifier: It is used for classification tasks where the goal is to predict the class of a sample. The majority class among the k-nearest neighbors determines the predicted class.\n",
    "\n",
    "KNN Regressor: It is used for regression tasks where the goal is to predict a continuous value. The predicted value is typically the average of the target values of the k-nearest neighbors. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2fad2e-9a58-4442-b3bf-4909b939cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q4. How do you measure the performance of KNN? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Common performance metrics for KNN include accuracy, precision, recall, F1-score for classification, and mean squared error or R-squared for regression. Cross-validation is often used to get a more reliable estimate of the model's performance. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c4a5e-86bf-4853-b078-19935db6c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q5. What is the curse of dimensionality in KNN? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" The curse of dimensionality refers to the phenomenon where the feature space becomes increasingly sparse as the number of dimensions (features) increases. In KNN, this can lead to a loss of effectiveness because, as the number of dimensions grows, the distance between points increases, and the concept of \"closeness\" becomes less meaningful. This can result in increased computational requirements and decreased performance. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4f8ec-2686-4da3-974c-5105fe36e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q6. How do you handle missing values in KNN? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" KNN can be sensitive to missing values. Imputation methods such as replacing missing values with the mean, median, or mode can be applied. Alternatively, you can modify the distance metric to handle missing values more effectively. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ff70e-4568-456f-b65f-4e49fb554495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" KNN Classifier is suitable for problems where the output is categorical, and you want to predict the class of a given data point. KNN Regressor is used when the output is continuous, and you want to predict a numerical value. The choice depends on the nature of the problem - whether it's a classification or regression problem. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec36269-0808-47ed-9322-255f06039add",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed? \"\"\"\n",
    "\n",
    "\n",
    "# ans\n",
    "\"\"\" Strengths:\n",
    "\n",
    "Simple and easy to understand.\n",
    "No training phase (lazy learning).\n",
    "Can adapt to changes in the dataset.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "Sensitive to irrelevant features.\n",
    "Computationally expensive for large datasets.\n",
    "Performance can degrade in high-dimensional spaces.\n",
    "\n",
    "Addressing weaknesses:\n",
    "\n",
    "Feature selection to reduce dimensionality.\n",
    "Optimizing the algorithm's parameters, such as choosing an appropriate K.\n",
    "Using dimensionality reduction techniques. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c747f-c434-4c25-8ac5-d883d038aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q9. What is the difference between Euclidean distance and Manhattan distance in KNN? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Euclidean distance: It is the straight-line distance between two points in Euclidean space. In KNN, it is commonly used as a distance metric.\n",
    "\n",
    "Manhattan distance: Also known as L1 distance or city block distance, it is the sum of the absolute differences between the coordinates of two points. It considers only horizontal and vertical movements. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc6c8a-aa2c-49ae-a07e-213a7842c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q10. What is the role of feature scaling in KNN? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Feature scaling is essential in KNN because the algorithm is based on distance measures. Features with large scales can dominate the distance calculations, leading to biased results. Common scaling methods include Min-Max scaling (normalization) and Z-score scaling (standardization). Scaling ensures that all features contribute equally to the distance computation, improving the performance of the algorithm. \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
